{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed0190",
   "metadata": {},
   "source": [
    "Usually RNNs are depicted using tanh activation for the hidden state and then simple linear map from h to output and then maybe a softmax to get a probability distribution and things like that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedfbb1",
   "metadata": {},
   "source": [
    "Let the matrix used for hidden states be W, for input be U and for output be V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b6b58",
   "metadata": {},
   "source": [
    "A torch class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3adac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.U = nn.Parameter(torch.randn(hidden_size, input_size))\n",
    "        self.W = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.V = nn.Parameter(torch.randn(output_size, hidden_size))\n",
    "        self.in_bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.out_bias = nn.Parameter(torch.zeros(output_size))\n",
    "        \n",
    "       \n",
    "        self._init_weights()\n",
    "        \n",
    "       \n",
    "        self.register_buffer('state', torch.zeros(hidden_size))\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Proper Xavier initialization\"\"\"\n",
    "        with torch.no_grad():\n",
    "            std_u = np.sqrt(2.0 / (self.U.size(1) + self.U.size(0)))\n",
    "            self.U.normal_(0, std_u)\n",
    "            \n",
    "            std_w = np.sqrt(2.0 / (2 * self.W.size(0)))\n",
    "            self.W.normal_(0, std_w)\n",
    "            \n",
    "            std_v = np.sqrt(2.0 / (self.V.size(1) + self.V.size(0)))\n",
    "            self.V.normal_(0, std_v)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.state = torch.tanh(\n",
    "            torch.matmul(self.U, input) + \n",
    "            torch.matmul(self.W, self.state) + \n",
    "            self.in_bias\n",
    "        )\n",
    "        \n",
    "        output = torch.matmul(self.V, self.state) + self.out_bias\n",
    "        \n",
    "        output = F.softmax(output, dim=-1)\n",
    "        \n",
    "        return output\n",
    "    def bptt():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d04fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):\n",
    "     def __init__(self):\n",
    "        super().__init__()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
